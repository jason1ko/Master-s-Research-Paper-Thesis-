# Regularized LDA in High Dimension: A Random Matrix Approach

This repository accompanies my master's research paper, which studies **regularized discriminant analysis (rLDA)** in the **high-dimensional, low-sample-size (HDLSS)** regime using tools from **random matrix theory (RMT)**.  
It contains selected **R codes** that reproduce the theoretical derivations, numerical experiments, and figures presented in the paper.

---

## ğŸ“˜ Paper Overview

**Title:** *Regularized Discriminant Analysis in the HDLSS Regime: A Random Matrix Perspective*  
**Author:** Han Jun Ko  
**Date:** January 2025  
**Institution:** Yonsei University, Department of Statistics  

### Abstract (Summary)

This work analyzes the asymptotic classification error of ridge-regularized LDA in the HDLSS regime.  
We establish explicit limiting forms for the misclassification risk:

$$
\mathrm{Err}(\hat{w}_\lambda) \to \Phi(-\Theta(\lambda))
$$

and provide theoretical and numerical characterization of the optimal regularization parameter Î» as a function of the aspect ratio  
Î³ = p/n and correlation strength Ï in Toeplitz covariance structures (AR(1), MatÃ©rn).

---

## ğŸ§® Key Contributions

1. **Closed-form limit under Î£ = I**  
   - Derived a piecewise closed-form expression for the MDP limit Î˜(0) across regimes Î³ < 1, Î³ = 1, Î³ > 1.  
   - Theoretical error curve Î¦(âˆ’Î˜(0)) peaks at Î³ = 1, reconciling classical HDLSS observations.

2. **Toeplitz Covariance Validation**  
   - Verified via **SzegÅ‘â€™s theorem** that AR(1) and MatÃ©rn covariances satisfy RMT spectral assumptions.

3. **Stable Approximation Pipeline**  
   - Implemented a numerically stable computation of Î˜(Î») using empirical spectral sums.  
   - Mapped **Î»_opt(Î³, Ï)** and analyzed the transition between MDP (Î»=0) and MD (Î»=âˆ).

---

## ğŸ§  Theoretical Framework

Regularized LDA forms a continuum between the **Maximal Data Piling (MDP)** and **Mean Difference (MD)** rules:

$$
v(\lambda) = (CC^\top + \lambda I)^{-1} w, \quad \lambda > 0.
$$

As Î» varies:
- Î» = 0 â†’ Maximal Data Piling (MDP)
- Î» = âˆ â†’ Mean Difference (MD)
- Intermediate Î» â†’ Ridge-regularized direction connecting both ends

The limiting misclassification risk depends on the **Stieltjes transform** of the sample covariance spectral distribution (Marchenkoâ€“Pastur law).

---

## ğŸ“Š Repository Contents

| File | Description |
|------|--------------|
| `ar1_example.R` | plot of $\Theta(\lambda)$, from Dobriban, E. & Wager, S. (2018) |
| `rda_functions.R` | calculate Stieltjes functions, from Dobriban, E. & Wager, S. (2018) |
| `rda_rcpp2.cpp` | generate multivariate Normal using Rcpp |
| `results.R` | includes all codes and functions needed for the paper |

---

## ğŸ” Numerical Patterns Observed

- Î»_opt **increases** with Î³ and **decreases** with Ï.  
- Î˜(Î»_opt) is close to Î˜(0) at small Î³ and approaches Î˜(âˆ) as Î³ grows.  
- Under MatÃ©rn covariance, Î¦(âˆ’Î˜(Î»_opt)) and Î¦(âˆ’Î˜(âˆ)) nearly coincide, indicating smoother effective regularization.

---

## ğŸ”— Key References

- Dobriban, E. & Wager, S. (2018). High-dimensional asymptotics of prediction: Ridge regression and classification. Annals of Statistics.
- Ahn, J. & Marron, J. S. (2010). The maximal data piling direction for discrimination. Biometrika.
- Lee, M. H., Ahn, J., & Jeon, Y. (2013). HDLSS discrimination with adaptive data piling. JCGS.
- Bai, Z. & Silverstein, J. (2010). Spectral Analysis of Large Dimensional Random Matrices. Springer.

---
